# ParkingUCaldasBot

## ðŸ¤– Manual Funcional del Prompt para Chatbot de WhatsApp (n8n)

Este manual describe el propÃ³sito, los parÃ¡metros, el comportamiento esperado y la integraciÃ³n del **prompt principal** que define la personalidad y las reglas del chatbot.

---

### 1. ðŸŽ¯ Objetivo del Prompt

El objetivo principal de este prompt es establecer el **rol, tono y las restricciones** del chatbot para asegurar una interacciÃ³n consistente, Ãºtil y dentro de los lÃ­mites de la integraciÃ³n con n8n y WhatsApp.

### 2. ðŸ“ Estructura del Prompt (Componentes Clave)

El prompt estÃ¡ compuesto por varias secciones que deben ser claras y jerÃ¡rquicas:

| SecciÃ³n | DescripciÃ³n | Contenido TÃ­pico |
| :--- | :--- | :--- |
| **A. Identidad y Rol** | Define quiÃ©n es el chatbot y cuÃ¡l es su trabajo principal. | "Eres un Asistente Virtual amable y profesional para [Nombre de la Empresa]. Tu tarea es responder preguntas frecuentes sobre [Producto/Servicio] y guiar al usuario a la secciÃ³n de [AcciÃ³n Principal, e.g., Compras]." |
| **B. Reglas y Restricciones** | Las directrices mÃ¡s importantes que el modelo **no debe** romper (lÃ­mites de contexto, longitud, etc.). | *No* inventes informaciÃ³n. Si no tienes la respuesta, pide disculpas y sugiere una alternativa. *Nunca* uses emojis que no sean [Lista de emojis permitidos]. MantÃ©n las respuestas por debajo de [NÃºmero] palabras. |
| **C. Contexto o InformaciÃ³n Fuente** | La base de conocimiento con la que debe trabajar el chatbot (puede ser insertada dinÃ¡micamente). | "La empresa ofrece tres planes: BÃ¡sico (\$10), Premium (\$25), y Empresarial (\$50). El soporte es de Lunes a Viernes." |
| **D. Formato de Salida (Output Format)** | La estructura que el chatbot debe seguir para que n8n pueda procesar la respuesta o para mejorar la legibilidad en WhatsApp. | Responde siempre con un tÃ­tulo en **negritas** y luego la respuesta en pÃ¡rrafos cortos. |

### 3. âš™ï¸ IntegraciÃ³n con n8n (Flujo Funcional)

El prompt interactÃºa con el flujo de n8n en las siguientes fases:

#### 3.1. Nodo de Entrada (Webhook/WhatsApp Trigger)

* **FunciÃ³n:** Recibe el mensaje (`user_message`) del usuario.
* **n8n AcciÃ³n:** Este mensaje se almacena en una variable, por ejemplo, `${{$json.body.message.text}}`.

#### 3.2. Nodo de Pre-Procesamiento (Code/Function)

* **FunciÃ³n:** Se utiliza para **construir el prompt final** concatenando las secciones estÃ¡ticas (A, B, D) con el contexto dinÃ¡mico (C) y el mensaje del usuario.
* **Variable de Salida Clave:** `final_prompt` (Contiene la instrucciÃ³n completa para el LLM).

$$\text{final\_prompt} = \text{Identidad} + \text{Reglas} + \text{Contexto DinÃ¡mico} + \text{"Usuario pregunta: "} + \text{user\_message}$$

#### 3.3. Nodo de LLM (AI/OpenAI/Custom Model)

* **FunciÃ³n:** EnvÃ­a el `final_prompt` al modelo de lenguaje (LLM).
* **ParÃ¡metros Clave:**
    * **Input:** `final_prompt`
    * **Temperatura:** (Suele ser baja, $T \approx 0.2$ para un comportamiento mÃ¡s predecible y fÃ¡ctico).
    * **Modelo:** (e.g., `gpt-3.5-turbo`, `gpt-4`).

#### 3.4. Nodo de Post-Procesamiento (Code/Function - Opcional)

* **FunciÃ³n:** Validar y limpiar la respuesta del LLM antes de enviarla.
    * *Ejemplo:* Recortar la respuesta si supera un lÃ­mite de caracteres especÃ­fico de WhatsApp, o aÃ±adir un `[Enviado por bot]` para trazabilidad.

#### 3.5. Nodo de Salida (WhatsApp Send Message)

* **FunciÃ³n:** EnvÃ­a la respuesta final al usuario.
* **Input:** La respuesta generada por el LLM.

### 4. âš ï¸ Casos LÃ­mite y Manejo de Errores

Para que el prompt sea robusto, debe anticipar fallas:

| Evento | InstrucciÃ³n en el Prompt (SecciÃ³n B) | n8n Manejo de Errores |
| :--- | :--- | :--- |
| **Consulta Irrelevante** | "Si la pregunta no tiene relaciÃ³n con nuestros productos/servicios, responde cortÃ©smente que solo puedes ayudar con temas de [Dominio]." | Uso de un nodo **IF** que detecte palabras clave (`gracias`, `adiÃ³s`) para terminar el flujo sin llamar al LLM, ahorrando coste. |
| **Contexto Agotado** | "Si la respuesta requiere informaciÃ³n no proporcionada en el contexto, *debes* responder: 'Lo siento, no tengo esa informaciÃ³n. Â¿Deseas hablar con un agente?'" | El LLM debe generar este texto, que luego puede disparar un nuevo flujo en n8n para notificar a un humano. |
| **Fallo del LLM** | (No aplicable al prompt, es un error tÃ©cnico) | **Error Handling Branch** en n8n: Si el nodo de LLM falla, el flujo debe saltar a un nodo que envÃ­e un mensaje genÃ©rico: "Estamos teniendo problemas tÃ©cnicos. Por favor, intÃ©ntalo mÃ¡s tarde." |

---

### 5. âœ… Recomendaciones de OptimizaciÃ³n

* **Prompt Cero (System Role):** Utiliza la funciÃ³n de "System Role" si tu LLM lo permite para las secciones A y B. Esto asegura que el modelo **siempre** mantiene la personalidad.
* **Longitud de Contexto:** Monitorea el uso de tokens. Si el contexto (SecciÃ³n C) crece mucho, considera usar una base de datos vectorial (como Pinecone o Weaviate) integrada con n8n para hacer una bÃºsqueda semÃ¡ntica *antes* de construir el `final_prompt` (RAG - Retrieval-Augmented Generation).

---

Â¡Excelente ajuste! Al especificar el modelo (**Gemini 2.5 Flash**), podemos refinar la terminologÃ­a del manual para que refleje mejor la configuraciÃ³n de Google AI, especialmente en lo que respecta al "System Instruction" y la gestiÃ³n de la conversaciÃ³n.

AquÃ­ estÃ¡ el manual funcional actualizado en formato Markdown para GitHub, sustituyendo las referencias genÃ©ricas de LLM por la terminologÃ­a de Gemini:

---

## 4. ðŸš€ OptimizaciÃ³n EspecÃ­fica para Gemini

### 4.1. Eficiencia con 2.5 Flash
El modelo `gemini-2.5-flash` estÃ¡ optimizado para velocidad y coste, lo que lo hace ideal para flujos de chat en tiempo real como WhatsApp.

* **Aprovecha la Ventana de Contexto:** 2.5 Flash tiene una ventana de contexto grande. Si el chat es conversacional, puedes inyectar los **Ãºltimos 3 pares de mensajes** como historial en el *Content* de la API para mantener la coherencia sin necesidad de una lÃ³gica de estado compleja.

### 4.2. InyecciÃ³n de Contexto (RAG)
Para bases de conocimiento extensas, se sigue recomendando el patrÃ³n RAG (Retrieval-Augmented Generation) para mantener el costo y la latencia bajos:

1.  **BÃºsqueda:** El `user_message` se usa para buscar los fragmentos de contexto mÃ¡s relevantes.
2.  **InyecciÃ³n:** Estos fragmentos se aÃ±aden a la **SecciÃ³n C** (Contexto) de la llamada, justo antes de la pregunta del usuario.

> **Formato de Contenido Sugerido:**
> `[CONTEXTO RELEVANTE]: [Fragmento 1]; [Fragmento 2].`
> `USUARIO PREGUNTA: [user_message]`

---
